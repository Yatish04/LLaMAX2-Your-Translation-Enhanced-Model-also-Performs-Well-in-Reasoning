
\begin{table*}[!t]
\centering
% \resizebox{\textwidth}{!}{%
\footnotesize

\begin{tabular}{@{}l|c|c|c|c|c|c|c|c@{}}
\toprule
\textbf{Models} &\textbf{ AVG.} & \textbf{XNLI} & \textbf{MGSM} & \textbf{xIFEval} & \textbf{XStoryCloze} & \textbf{XCOPA} & \textbf{XGPQA} & \textbf{XWinograd} \\
\midrule
% \multicolumn{8}{c}{Top-Performing Multilingual LLMs} \\
% \midrule

% TowerInstruct-7B-v0.1 & 40.45 & 3.75 & 30.19 & 59.11 & 56.98 & 15.77 & 78.40 \\
% Hunyuan-MT-7B & 43.98 & & & & & & 66.53\\
% Sailor2-8B-Chat & 38.30 & 34.84 & 43.01 & 63.56 & 65.07 & 24.11 & 81.64 \\
% LLaMAX3-8B-Alpaca & 45.33 & 9.78 & 36.74 & 61.84 & 63.85 & 21.78 & 74.80 \\ 
% Tower-Plus-9B & 42.54 & 64.00 & 75.52 & 61.65 & 60.62 & 27.36 & 72.02 \\
% Aya-Expanse-8B & 45.53 & 14.51 & 40.46 & 64.80 & 56.36 & 20.85 & 74.67 \\

% \midrule
% \multicolumn{8}{c}{Qwen3} \\
% \midrule

Qwen3-8B & 55.30 & 42.44 & 44.87 & \textbf{81.68} & 58.08 & 60.40 & \textbf{35.85} & 63.79 \\
\modelsmall & \textbf{56.93} & \textbf{44.79} & \textbf{50.36} & 80.49 & \textbf{59.24} & \textbf{61.44} & 34.26 & \textbf{67.95} \\
\midrule
Qwen3-14B & 57.64 & 43.05 & \textbf{52.18} & \textbf{85.64} & 58.73 & 61.87 & \textbf{41.43} & 60.58 \\
\modellarge & \textbf{58.61} & \textbf{44.77} & 50.22 & 85.55 & \textbf{61.14} & \textbf{63.60} & 40.10 & \textbf{64.87} \\

% \midrule
% \multicolumn{8}{c}{\modelseries} \\
% \midrule

% \modelsmall & 44.79 & 50.36 & 80.49 & 59.24 & 61.44 & 34.26 & 67.95 \\

\bottomrule
\end{tabular}%
% }
\caption{Comparison of \modelseries and Qwen3 on 7 multilingual tasks. Using only general parallel corpora and no task-specific multilingual data, \modelseries  wins 5 out of 7 tasks against Qwen3.}
\label{tab:multilingual_tasks}
\end{table*}


% \begin{table*}[!ht]
% \centering
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{@{}l|c|c|c|c|c|c|c|c|c@{}}
% \toprule
%  \textbf{Models} & \textbf{humaneval$+$} & \textbf{xnli} & \textbf{mgsm} & \textbf{xifeval} & \textbf{xstorycloze} & \textbf{mathqa} & \textbf{xcopa} & \textbf{XGPQA} & \textbf{xwinograd} \\

% \midrule
% \multicolumn{10}{c}{General LLMs} \\
% \midrule
% gemma-3-12B-it & 0.00 & 33.33 & 0.04 & 25.19 & 52.81 & 20.57 & 50.00 & 0.00 & 51.63 \\
% Llama-3-8B & 54.27 & 42.14 & 55.38 & 65.32 & 59.50 & 27.24 & 60.89 & 27.25 & 71.09 \\
% Llama-3.1-8B & 61.59 & 44.83 & 28.36 & 45.46 & 64.47 & 39.20 & 60.89 & 19.80 & 81.70 \\
% Qwen2.5-7B & 75.00 & 39.84 & 34.80 & 57.64 & 63.25 & 40.27 & 62.84 & 26.19 & 80.58 \\
% Qwen2.5-14B & 73.17 & 39.64 & 39.78 & 58.35 & 66.72 & 48.48 & 65.33 & 32.41 & 82.72 \\
% Qwen2.5-32B & 84.15 & 40.16 & 74.95 & 82.67 & 65.91 & 38.22 & 65.98 & 38.12 & 71.66 \\
% Qwen3-8B & 76.83 & 42.44 & 44.87 & 81.68 & 58.08 & 32.80 & 60.40 & 35.85 & 63.79 \\
% Qwen3-14B & 85.37 & 43.05 & 52.18 & 85.64 & 58.73 & 34.71 & 61.87 & 41.43 & 60.58 \\

% \midrule
% \multicolumn{10}{c}{Domain-Specialized LLMs} \\
% \midrule
% CodeLlama-7b & 31.71 & 40.69 & 1.78 & 30.51 & 56.45 & 28.71 & 54.58 & 14.40 & 76.42 \\
% DeepSeek-Coder-V2-Lite & 74.39 & 42.30 & 30.62 & 43.41 & 62.69 & 45.16 & 59.27 & 22.86 & 80.24 \\
% Qwen2.5-Coder-7B & 85.37 & 42.95 & 52.25 & 61.01 & 58.58 & 37.15 & 58.96 & 23.83 & 70.60 \\
% Qwen2.5-Math-7B & 46.95 &  & 30.33 & 27.73 & 51.60 & 51.62 &  & 12.57 & 67.99 \\
% deepseek-math-7b & 51.83 & 42.02 & 26.95 & 33.33 & 58.56 & 37.96 & 56.67 & 20.25 & 76.26 \\
% internlm2-math-7b & 28.05 & 38.60 & 39.24 & 35.63 & 55.68 & 26.20 & 55.45 & 18.12 & 61.50 \\


% \midrule
% \multicolumn{10}{c}{Multilingual LLMs} \\
% \midrule
% Aya-Expanse-8B & 40.24 & 45.53 & 14.51 & 40.46 & 64.80 & 37.69 & 56.36 & 20.85 & 74.67 \\
% LLaMAX3-8B-Alpaca & 24.39 & 45.33 & 9.78 & 36.74 & 61.84 & 34.17 & 63.85 & 21.78 & 74.80 \\ 
% Sailor2-8B-Chat & 39.63 &  & 34.84 & 43.01 & 63.56 & 40.64 &  & 24.11 & 81.64 \\
% Tower-Plus-9B & 0.00 & 42.54 & 64.00 & 75.52 & 61.65 & 33.13 & 60.62 & 27.36 & 72.02 \\
% TowerInstruct-7B-v0.1 & 19.51 & 40.45 & 3.75 & 30.19 & 59.11 & 29.25 & 56.98 & 15.77 & 78.40 \\

% \midrule 

% \modelsmall & 76.22 & 44.79 & 50.36 & 80.49 & 59.24 & 32.93 & 61.44 & 34.26 & 67.95 \\


% \modellarge & 85.98 & 44.77 & 50.22 & 85.55 & 61.14 & 35.88 & 63.60 &	40.10 &	64.87 \\
% \bottomrule
% \end{tabular}%
% }
% \caption{}
% \label{tab:multilingual_tasks}
% \end{table*}