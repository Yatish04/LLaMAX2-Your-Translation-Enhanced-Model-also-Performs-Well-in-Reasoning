\begingroup
\renewcommand{\arraystretch}{1} % Default value: 1
\begin{table*}[!ht]
\centering
\footnotesize
\resizebox{\linewidth}{!}{
\begin{tabular}{l|rc|rc|rc|rc|rc|rc|rc}


\toprule
% \rowcolor{gray!10} 
& \multicolumn{2}{c|}{\textbf{x $\rightarrow$ en}} & \multicolumn{2}{c|}{\textbf{x $\rightarrow$ sw}} & \multicolumn{2}{c|}{\textbf{x $\rightarrow$ th}} & \multicolumn{2}{c|}{\textbf{x $\rightarrow$ bn}} & \multicolumn{2}{c|}{\textbf{x$\rightarrow$ zh}} & \multicolumn{2}{c|}{\textbf{x $\rightarrow$ ar}} & \multicolumn{2}{c}{\textbf{x $\rightarrow$ ko}} \\
% \rowcolor{gray!10}  
& \textbf{spBLEU} & \textbf{xComet} & \textbf{spBLEU} & \textbf{xComet} & \textbf{spBLEU} & \textbf{xComet} & \textbf{spBLEU} & \textbf{xComet} & \textbf{spBLEU} & \textbf{xComet} & \textbf{spBLEU} & \textbf{xComet} & \textbf{spBLEU} & \textbf{xComet} \\

\midrule
% \rowcolor{gray!10} 
\multicolumn{15}{c}{Super-Large Models} \\
\midrule

% \rowcolor{gray!10}
DeepSeek-R1-0528 & 1.85 & 14.73 & 0.57 & 13.98 & 5.01 & 28.10 & 0.33 & 14.92 & 19.84 & 67.62 & 1.53 & 16.87 & 1.35 & 16.68 \\
% \rowcolor{gray!10}
DeepSeek-V3-0324 & 33.59 & 79.36 & 16.60 & 47.84 & 24.37 & 60.62 & 16.5 & 60.33 & 29.55 & 86.96 & 9.27 & 40.02 & 14.40 & 59.19 \\
% \rowcolor{gray!10}
Kimi-K2 & 40.47 & 95.16 & 22.38 & 67.43 & 36.81 & 87.62 & 23.70 & 87.52 & 31.86 & 88.44 & 28.09 & 87.96 & 22.57 & 89.80 \\
% \rowcolor{gray!10}
Qwen3-235B-A22B & 41.19 & 94.80 & 16.01 & 41.90 & 35.81 & 87.51 & 24.37 & 82.90 & 31.39 & 87.98 & 27.92 & 87.11 & 21.59 & 88.85 \\


\midrule
% \rowcolor[HTML]{fff8e2} 
\multicolumn{15}{c}{General LLMs} \\
\midrule

% Gemma-3-1b-it & 23.52 & 66.84 & 0.60 & 12.99 & 4.55 & 22.54 & 2.69 & 30.36 & 11.45 & 50.66 & 1.07 & 12.55 & 3.92 & 28.5 \\
% Gemma-3-4b-it & 23.37 & 57.99 & 0.26 & 14.3 & 1.98 & 18.96 & 0.28 & 15.08 & 12.13 & 43.54 & 0.07 & 14.52 & 2.02 & 21.91 \\
% \rowcolor[HTML]{fff8e2}  
Gemma-3-12B-it & 36.82 & 86.44 & 0.63 & 13.46 & 2.00 & 19.13 & 0.90 & 18.61 & 22.36 & 69.97 & 1.20 & 17.63 & 7.41 & 39.49 \\
% \rowcolor[HTML]{fff8e2} 
LLaMA3-8B & 0.38 & 14.98 & 2.38 & 17.27 & 1.45 & 18.1 & 6.46 & 33.12 & 1.79 & 21.52 & 2.78 & 18.07 & 0.13 & 11.11 \\
% \rowcolor[HTML]{fff8e2} 
LLaMA3.1-8B & 30.96 & 79.59 & 5.83 & 32.84 & 16.55 & 50.92 & 13.80 & 59.67 & 17.35 & 64.60 & 9.42 & 40.48 & 11.02 & 60.44 \\
% Qwen2.5-1.5B & 13.84 & 42.09 & 0.05 & 14.73 & 1.16 & 19.05 & 0.03 & 14.17 & 12.60 & 50.65 & 3.23 & 19.85 & 0.76 & 18.58 \\
% Qwen2.5-3B & 27.12 & 72.59 & 0.12 & 11.38 & 16.24 & 44.24 & 1.83 & 18.08 & 20.25 & 67.23 & 9.21 & 43.39 & 7.77 & 46.72 \\
% \rowcolor[HTML]{fff8e2} 
Qwen2.5-7B & 31.07 & 78.37 & 1.70 & 9.62 & 20.46 & 55.45 & 6.11 & 28.07 & 23.13 & 73.58 & 12.94 & 50.75 & 8.94 & 53.13 \\
% \rowcolor[HTML]{fff8e2} 
Qwen2.5-14B & 25.89 & 64.86 & 1.58 & 13.00 & 8.68 & 37.33 & 6.50 & 31.96 & 22.75 & 69.12 & 7.98 & 33.72 & 6.64 & 38.73 \\
% \rowcolor[HTML]{fff8e2} 
Qwen2.5-32B & 36.2 & 87.64 & 5.41 & 15.70 & 27.44 & 69.19 & 13.48 & 55.83 & 27.68 & 82.02 & 17.51 & 61.38 & 15.86 & 70.59 \\

% Qwen3-0.6B & 19.49 & 58.82 & 0.19 & 21.88 & 8.73 & 24.95 & 0.15 & 17.95 & 12.02 & 49.71 & 2.84 & 16.77 & 2.52 & 22.41 \\
% Qwen3-1.7B & 28.08 & 78.98 & 0.29 & 13.46 & 17.54 & 48.66 & 1.31 & 19.79 & 19.93 & 69.54 & 11.36 & 41.49 & 8.24 & 48.62 \\
% Qwen3-4B & 32.91 & 86.19 & 2.62 & 10.20 & 25.74 & 67.33 & 9.30 & 43.07 & 24.65 & 77.80 & 18.09 & 65.11 & 13.17 & 68.54 \\
% \rowcolor[HTML]{fff8e2} Qwen3-8B & 35.24 & 89.89 & 3.49 & 12.52 & 29.85 & 75.47 & 14.14 & 59.91 & 26.88 & 81.65 & 21.73 & 75.18 & 16.20 & 76.48 \\
% \rowcolor[HTML]{fff8e2} Qwen3-14B & 36.92 & 91.98 & 5.87 & 15.33 & 32.40 & 80.12 & 17.50 & 68.09 & 28.71 & 83.95 & 24.01 & 79.75 & 18.77 & 82.19 \\
% \rowcolor[HTML]{fff8e2} Qwen3-32B & 38.05 & 92.60 & 8.18 & 19.58 & 33.02 & 81.04 & 19.09 & 72.6 & 29.05 & 84.50 & 24.10 & 79.37 & 19.34 & 83.26 \\

\midrule

% \rowcolor{green!10}  
\multicolumn{15}{c}{Domain-Specialized LLMs} \\
\midrule

% \rowcolor{green!10} 
InternLM2-Math-7B & 21.67 & 64.71 & 0.29 & 24.34 & 0.19 & 28.25 & 0.08 & 21.61 & 4.75 & 45.37 & 1.19 & 24.13 & 0.24 & 19.35 \\
% \rowcolor{green!10} 
Deepseek-Math-7B & 21.94 & 71.32 & 0.11 & 23.71 & 3.26 & 25.90 & 0.98 & 22.08 & 12.14 & 59.94 & 0.78 & 22.60 & 2.35 & 37.64 \\
% \rowcolor{green!10} 
DeepSeek-Coder-V2-Lite & 29.61 & 79.00 & 1.15 & 11.12 & 14.06 & 34.95 & 5.04 & 27.76 & 21.90 & 70.26 & 11.82 & 43.43 & 9.66 & 49.13 \\

% \rowcolor{green!10} 
CodeLlama-7b & 20.65 & 63.32 & 0.52 & 24.76 & 0.34 & 34.47 & 0.08 & 25.97 & 0.63 & 47.69 & 0.07 & 24.45 & 0.40 & 32.94 \\
% Qwen2.5-Math-1.5B & 0.20 & 11.65 & 0.04 & 12.53 & 0.02 & 14.04 & 0.02 & 14.72 & 0.02 & 12.16 & 0.01 & 11.61 & 0.03 & 11.09 \\
% \rowcolor{green!10} 
Qwen2.5-Math-7B & 2.29 & 17.48 & 0.09 & 14.04 & 0.08 & 20.61 & 0.02 & 20.2 & 0.32 & 19.81 & 0.03 & 22.46 & 0.13 & 20.23 \\
% Qwen2.5-Coder-1.5B & 16.46 & 50.25 & 0.05 & 19.92 & 1.41 & 19.00 & 0.13 & 16.60 & 10.55 & 44.27 & 0.88 & 16.35 & 0.89 & 20.58 \\
% Qwen2.5-Coder-3B & 25.38 & 68.16 & 0.05 & 13.94 & 5.30 & 32.27 & 0.37 & 15.07 & 17.25 & 60.76 & 3.68 & 31.18 & 2.73 & 36.22 \\
% \rowcolor{green!10} 
Qwen2.5-Coder-7B & 28.52 & 73.67 & 0.19 & 12.23 & 12.14 & 42.03 & 1.32 & 20.60 & 20.80 & 66.86 & 9.31 & 41.37 & 7.14 & 45.93 \\
% \rowcolor{green!10} 
Qwen2.5-Coder-14B & 30.16 & 75.40 & 0.67 & 13.52 & 7.95 & 36.35 & 1.06 & 24.91 & 21.13 & 73.30 & 5.65 & 30.52 & 3.63 & 34.00 \\
% \rowcolor{green!10} 
Qwen2.5-Coder-32B & 31.80 & 78.01 & 0.85 & 14.78 & 20.37 & 51.62 & 8.66 & 38.07 & 25.04 & 76.24 & 13.48 & 49.67 & 11.03 & 56.90 \\

\midrule
% \rowcolor{blue!10} 
\multicolumn{15}{c}{Multilingual LLMs} \\
\midrule
% \rowcolor{blue!10} 
TowerInstruct-7B-v0.1 & 29.26 & 72.56 & 0.71 & 37.34 & 0.48 & 53.90 & 0.25 & 59.70 & 17.02 & 62.16 & 0.48 & 58.62 & 13.4 & 62.28 \\
% \rowcolor{blue!10} 
Hunyuan-MT-7B & 21.20 & 67.68 & 5.55 & 32.95 & 17.70 & 56.92 & 8.92 & 47.17 & 18.35 & 73.67 & 13.70 & 54.37 & 10.32 & 58.28 \\
% \rowcolor{blue!10} 
Sailor2-8B-Chat & 0.52 & 17.81 & 0.67 & 17.09 & 24.12 & 60.84 & 2.42 & 20.67 & 16.69 & 60.53 & 5.60 & 31.41 & 4.46 & 37.03 \\
% \rowcolor{blue!10} 
LLaMAX3-8B-Alpaca & 35.96 & 89.98 & 10.00 & 53.15 & 23.62 & 72.43 & 12.04 & 66.76 & 21.08 & 77.72 & 17.57 & 72.17 & 11.90 & 76.11 \\
% \rowcolor{blue!10} 
Tower-Plus-9B & \textbf{40.12} & 91.74 & 2.45 & 20.80 & 18.71 & 53.76 & 2.47 & 58.16 & \textbf{30.37} & 82.96 & 9.66 & 48.73 & \textbf{22.36} & \textbf{85.53} \\
% \rowcolor{blue!10} 
Aya-Expanse-8B & 33.13 & 79.28 & 1.49 & 8.91 & 6.42 & 19.81 & 4.94 & 25.08 & 23.53 & 70.67 & 23.77 & 70.21 & 17.71 & 70.53 \\
% \rowcolor{blue!10} 
Aya-Expanse-32B & 39.72 & 88.63 & 2.60 & 16.53 & 15.16 & 40.65 & 11.93 & 53.76 & 27.93 & 80.70 & \textbf{28.63} & \textbf{81.70} & 21.71 & 82.79 \\

\midrule

% \rowcolor{pink!50}  
\multicolumn{15}{c}{\modelseries} \\
\midrule

% r1-0528 &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\
% qwen3-235B &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\
% Qwen3-32B &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\
% dv3-0324 &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\
% k2 &  &  &  &  &  &  &  &  &  &  &  &  &  & \\
% \rowcolor{pink!50} 
Qwen3-8B & 35.24 & 89.89 & 3.49 & 12.52 & 29.85 & 75.47 & 14.14 & 59.91 & 26.88 & 81.65 & 21.73 & 75.18 & 16.20 & 76.48 \\
% \rowcolor{pink!50}  
Qwen3-8B-FT & 9.73 & 34.21 & 2.19 & 20.31 & 2.36 & 22.56 & 5.02 & 24.63 & 9.1 & 32.25 & 8.16 & 37.12 & 2.19 & 21.57 \\
% \rowcolor{pink!50}  
Qwen3-8B-LoRA & 35.63 & 84.64 & 1.99 & 17.66 & 22.39 & 67.04 & 10.62 & 47.49 & 22.89 & 75.64 & 15.04 & 66.77 & 11.82 & 68.17 \\
% \rowcolor{pink!50} 
\modelsmall & 38.02 & 91.35 & 18.60 & 50.99 & 27.84 & 73.17 & 19.39 & 70.67 & 26.95 & 82.15 & 24.00 & 77.50 & 18.08 & 80.54 \\
\midrule
% \rowcolor{pink!50}  
Qwen3-14B & 36.92 & 91.98 & 5.87 & 15.33 & 32.40 & 80.12 & 17.50 & 68.09 & 28.71 & 83.95 & 24.01 & 79.75 & 18.77 & 82.19 \\
% \rowcolor{pink!50}  
Qwen3-14B-FFT & 40.37 & 90.99 & 13.04 & 53.69 & 19.34 & 63.68 & 17.13 & 67.70 & 24.98 & 77.93 & 21.37 & 73.84 & 14.76 & 74.66 \\
% \rowcolor{pink!50}  
Qwen3-14B-LoRA & 37.19 & 86.57 & 6.13 & 18.9 & 26.10 & 67.32 & 16.12 & 60.78 & 24.40 & 75.81 & 20.88 & 69.40 & 17.08 & 77.12 \\ 
% \rowcolor{pink!50} 
\modellarge & 39.01 & 92.86 & 20.02 & 57.66 & 32.03 & 80.47 & 21.63 & 77.43 & 28.96 & 84.90 & 26.31 & 82.19 & 20.31 & 84.68 \\
\bottomrule

\toprule
% \rowcolor{gray!10} 
& \multicolumn{2}{c|}{\textbf{en $\rightarrow$ x}} & \multicolumn{2}{c|}{\textbf{sw $\rightarrow$ x}} & \multicolumn{2}{c|}{\textbf{th $\rightarrow$ x}} & \multicolumn{2}{c|}{\textbf{bn $\rightarrow$ x}} & \multicolumn{2}{c|}{\textbf{zh $\rightarrow$ x}} & \multicolumn{2}{c|}{\textbf{ar $\rightarrow$ x}} & \multicolumn{2}{c}{\textbf{ko $\rightarrow$ x}} \\
% \rowcolor{gray!10} 
& \textbf{spBLEU} & \textbf{xComet} & \textbf{spBLEU} & \textbf{xComet} & \textbf{spBLEU} & \textbf{xComet} & \textbf{spBLEU} & \textbf{xComet} & \textbf{spBLEU} & \textbf{xComet} & \textbf{spBLEU} & \textbf{xComet} & \textbf{spBLEU} & \textbf{xComet} \\

\midrule
% \rowcolor{gray!10} 
\multicolumn{15}{c}{Super-Large Models} \\
\midrule

% \rowcolor{gray!10} 
DeepSeek-R1-0528 & 2.91 & 20.74 & 1.18 & 14.28 & 1.90 & 18.81 & 1.72 & 18.12 & 10.85 & 48.22 & 1.92 & 15.68 & 2.40 & 20.09 \\
% \rowcolor{gray!10} 
DeepSeek-V3-0324 & 30.83 & 74.27 & 15.84 & 43.38 & 20.20 & 64.89 & 21.47 & 65.26 & 25.52 & 84.73 & 24.31 & 66.01 & 23.44 & 72.22 \\
% \rowcolor{gray!10} 
Kimi-K2 & 38.78 & 93.86 & 29.92 & 72.68 & 26.37 & 88.32 & 27.93 & 83.90 & 27.07 & 91.84 & 30.38 & 87.33 & 27.45 & 87.26 \\
% \rowcolor{gray!10} 
Qwen3-235B-A22B & 36.84 & 91.37 & 27.34 & 66.69 & 26.54 & 85.61 & 27.19 & 81.47 & 26.42 & 88.48 & 29.68 & 84.62 & 26.80 & 84.10 \\

\midrule
% \rowcolor[HTML]{fff8e2} 
\multicolumn{15}{c}{General LLMs} \\
\midrule

% Gemma-3-1b-it & 5.11 & 27.87 & 1.25 & 11.01 & 2.92 & 24.77 & 3.43 & 23.21 & 4.58 & 29.71 & 3.38 & 22.10 & 2.89 & 22.54 \\ 
% Gemma-3-4b-it & 2.98 & 20.59 & 2.39 & 15.87 & 3.62 & 21.64 & 1.12 & 15.85 & 3.17 & 20.64 & 3.90 & 20.04 & 4.94 & 25.39 \\
% \rowcolor[HTML]{fff8e2} 
Gemma-3-12B-it & 7.23 & 44.02 & 4.42 & 20.07 & 7.06 & 30.65 & 6.86 & 31.32 & 8.99 & 39.85 & 9.60 & 34.06 & 9.33 & 36.39 \\
% \rowcolor[HTML]{fff8e2} 
LLaMA3-8B & 18.77 & 57.18 & 0.07 & 11.41 & 2.38 & 22.78 & 0.45 & 15.49 & 1.42 & 19.48 & 0.38 & 12.24 & 1.87 & 21.38 \\
% \rowcolor[HTML]{fff8e2}
LLaMA3.1-8B & 26.97 & 81.09 & 8.96 & 29.03 & 15.06 & 65.86 & 16.62 & 59.50 & 10.74 & 51.34 & 10.61 & 42.32 & 16.73 & 63.44 \\

% Qwen2.5-1.5B & 7.85 & 37.72 & 0.17 & 12.43 & 3.01 & 22.24 & 1.55 & 15.60 & 2.24 & 23.20 & 2.78 & 19.25 & 2.60 & 21.15 \\
% Qwen2.5-3B & 15.38 & 53.37 & 4.05 & 11.75 & 11.72 & 50.01 & 8.41 & 32.38 & 10.62 & 53.27 & 12.63 & 46.48 & 11.26 & 45.04 \\
% \rowcolor[HTML]{fff8e2} 
Qwen2.5-7B & 19.39 & 59.90 & 5.03 & 15.02 & 14.20 & 54.89 & 10.74 & 41.27 & 12.91 & 55.85 & 14.37 & 48.59 & 12.75 & 47.20 \\
% \rowcolor[HTML]{fff8e2} 
Qwen2.5-14B & 6.90 & 31.92 & 4.26 & 19.95 & 10.34 & 41.12 & 8.55 & 38.32 & 10.66 & 43.37 & 10.04 & 36.82 & 7.78 & 33.41 \\
% \rowcolor[HTML]{fff8e2} 
Qwen2.5-32B & 27.58 & 74.29 & 14.34 & 35.24 & 18.94 & 66.27 & 17.94 & 59.77 & 17.26 & 64.52 & 21.65 & 65.39 & 19.00 & 63.87 \\

% Qwen3-0.6B & 9.95 & 36.96 & 1.48 & 20.64 & 5.61 & 29.79 & 2.80 & 15.06 & 6.80 & 36.23 & 5.89 & 24.60 & 5.58 & 25.87 \\
% Qwen3-1.7B & 17.86 & 56.74 & 3.20 & 12.27 & 11.20 & 48.35 & 9.54 & 39.05 & 12.81 & 54.89 & 13.07 & 46.04 & 11.58 & 46.96 \\
% Qwen3-4B & 24.76 & 72.84 & 8.80 & 18.27 & 16.68 & 64.60 & 15.89 & 58.11 & 17.87 & 71.12 & 19.10 & 63.23 & 17.22 & 63.38 \\
% \rowcolor[HTML]{fff8e2} Qwen3-8B & 28.86 & 80.27 & 13.61 & 32.32 & 19.68 & 72.64 & 19.38 & 66.51 & 20.82 & 78.90 & 22.85 & 72.27 & 20.23 & 71.79 \\
% \rowcolor[HTML]{fff8e2} Qwen3-14B & 31.78 & 84.54 & 18.40 & 43.75 & 22.35 & 78.09 & 22.47 & 72.32 & 23.02 & 83.04 & 25.28 & 76.98 & 22.90 & 77.07 \\
% \rowcolor[HTML]{fff8e2} Qwen3-32B & 32.22 & 84.51 & 20.59 & 49.98 & 23.26 & 79.21 & 23.45 & 74.52 & 23.28 & 83.29 & 26.29 & 78.03 & 23.65 & 78.64 \\


\midrule
% \rowcolor{green!10} 
\multicolumn{15}{c}{Domain-Specialized LLMs} \\
 \midrule

% \rowcolor{green!10} 
CodeLlama-7B & 4.81 & 71.10 & 0.15 & 11.70 & 0.70 & 15.41 & 0.21 & 12.45 & 1.82 & 47.68 & 1.04 & 17.83 & 1.61 & 33.68 \\
% \rowcolor{green!10} 
InternLM2-Math-7B & 7.26 & 40.40 & 1.13 & 13.76 & 1.64 & 21.55 & 0.83 & 19.17 & 4.25 & 49.02 & 2.71 & 39.27 & 2.48 & 35.73 \\

% \rowcolor{green!10} 
DeepSeek-Math-7B & 12.97 & 53.45 & 0.79 & 15.72 & 3.36 & 38.27 & 2.49 & 30.42 & 6.17 & 55.35 & 3.78 & 31.81 & 5.52 & 50.14 \\
% \rowcolor{green!10} 
DeepSeek-Coder-V2-Lite & 21.82 & 63.34 & 5.82 & 17.29 & 10.66 & 45.73 & 9.95 & 36.05 & 13.66 & 58.76 & 13.57 & 48.63 & 12.48 & 47.68 \\

% Qwen2.5-Math-1.5B & 0.09 & 30.89 & 0.02 & 9.92 & 0.01 & 10.36 & 0.01 & 12.31 & 0.08 & 20.72 & 0.00 & 10.17 & 0.01 & 9.67 \\
% \rowcolor{green!10} 
Qwen2.5-Math-7B & 0.44 & 38.89 & 0.03 & 14.01 & 0.16 & 14.33 & 0.04 & 14.80 & 0.36 & 28.90 & 0.11 & 11.74 & 0.19 & 12.55 \\
% Qwen2.5-Coder-1.5B & 7.66 & 40.56 & 0.18 & 10.30 & 2.34 & 18.70 & 0.28 & 12.06 & 4.47 & 36.46 & 2.06 & 16.31 & 2.66 & 18.93 \\
% Qwen2.5-Coder-3B & 12.84 & 53.07 & 1.52 & 12.29 & 6.87 & 36.57 & 3.37 & 18.96 & 8.14 & 50.20 & 7.61 & 35.19 & 7.64 & 38.43 \\
% \rowcolor{green!10} 
Qwen2.5-Coder-7B & 17.53 & 59.59 & 3.96 & 13.81 & 10.13 & 43.65 & 7.33 & 28.57 & 11.26 & 56.78 & 10.70 & 41.13 & 10.73 & 44.55 \\
% \rowcolor{green!10} 
Qwen2.5-Coder-14B & 19.19 & 61.28 & 2.78 & 15.87 & 6.86 & 36.47 & 6.65 & 31.96 & 13.13 & 59.08 & 8.66 & 39.29 & 7.30 & 38.95 \\
% \rowcolor{green!10} 
Qwen2.5-Coder-32B & 20.94 & 62.04 & 8.00 & 23.64 & 11.47 & 47.78 & 11.20 & 42.60 & 15.61 & 61.88 & 13.46 & 47.14 & 13.19 & 49.97 \\



\midrule
% \rowcolor{blue!10} 
\multicolumn{15}{c}{Multilingual LLMs} \\
 \midrule

% \rowcolor{blue!10} 
TowerInstruct-7B-v0.1 & 18.26 & 67.80 & 2.67 & 17.33 & 3.82 & 29.22 & 1.78 & 21.56 & 10.89 & 69.26 & 7.95 & 39.09 & 11.26 & 70.69 \\
% \rowcolor{blue!10} 
Hunyuan-MT-7B & 28.43 & 87.96 & 14.12 & 39.86 & 7.53 & 36.37 & 4.60 & 28.71 & 20.37 & 83.94 & 15.72 & 55.10 & 14.31 & 51.93 \\
% \rowcolor{blue!10} 
Sailor2-8B-Chat & 16.03 & 54.11 & 1.76 & 14.08 & 4.30 & 33.86 & 5.72 & 30.13 & 3.83 & 28.77 & 7.29 & 34.26 & 7.18 & 36.11 \\
% \rowcolor{blue!10} 
LLaMAX3-8B-Alpaca & 26.62 & 83.22 & 20.23 & 59.06 & 16.03 & 74.88 & 17.20 & 68.49 & 16.51 & 80.33 & 18.37 & 73.77 & 17.39 & 72.23 \\
% \rowcolor{blue!10} 
Tower-Plus-9B & 28.83 & 79.85 & 18.38 & 46.19 & 19.02 & 70.29 & 18.64 & 62.92 & 19.39 & 75.55 & 21.72 & 69.28 & 20.68 & 71.93 \\
% \rowcolor{blue!10} 
Aya-Expanse-8B & 25.75 & 68.36 & 7.90 & 16.43 & 11.39 & 40.78 & 11.29 & 36.77 & 17.85 & 65.45 & 20.21 & 60.86 & 18.41 & 60.74 \\
% \rowcolor{blue!10} 
Aya-Expanse-32B & 30.21 & 78.30 & 16.72 & 38.82 & 18.25 & 64.11 & 19.37 & 62.04 & 21.26 & 75.21 & 24.71 & 71.16 & 22.07 & 70.84 \\


\midrule
% \rowcolor{pink!50} 
\multicolumn{15}{c}{\modelseries} \\
\midrule
% \rowcolor{pink!50}
Qwen3-8B & 28.86 & 80.27 & 13.61 & 32.32 & 19.68 & 72.64 & 19.38 & 66.51 & 20.82 & 78.90 & 22.85 & 72.27 & 20.23 & 71.79 \\
% \rowcolor{pink!50} 
Qwen3-8B-FFT & 10.73 & 35.25 & 6.81 & 24.55 & 4.34 & 23.75 & 5.88 & 25.34 & 6.25 & 30.0 & 7.13 & 28.00 & 5.69 & 25.41 \\
% \rowcolor{pink!50} 
Qwen3-8B-LoRA & 26.57 & 73.86 & 10.40 & 29.33 & 17.27 & 65.55 & 16.78 & 58.28 & 17.93 & 71.24 & 19.77 & 64.89 & 17.37 & 63.33 \\
% \rowcolor{pink!50} 
\modelsmall & 32.82 & 85.52 & 21.41 & 55.06 & 22.68 & 77.36 & 22.47 & 71.75 & 23.31 & 83.13 & 25.46 & 75.63 & 22.73 & 75.77 \\
\midrule
% \rowcolor{pink!50} 
Qwen3-14B & 31.78 & 84.54 & 18.40 & 43.75 & 22.35 & 78.09 & 22.47 & 72.32 & 23.02 & 83.04 & 25.28 & 76.98 & 22.90 & 77.07 \\
% \rowcolor{pink!50} 
Qwen3-14B-FFT & 34.35 & 83.58 & 21.26 & 57.83 & 16.62 & 71.63 & 20.07 & 68.88 & 21.17 & 80.57 & 23.42 & 73.69 & 20.38 & 71.66 \\
% \rowcolor{pink!50} 
Qwen3-14B-LoRA & 28.26 & 75.52 & 15.79 & 40.48 & 20.39 & 70.27 & 18.78 & 61.55 & 18.36 & 69.12 & 22.61 & 68.99 & 21.34 & 71.84 \\
% \rowcolor{pink!50} 
\modellarge & 35.97 & 89.51 & 23.19 & 57.23 & 24.91 & 83.40 & 24.93 & 77.15 & 25.41 & 87.64 & 28.18 & 81.90 & 25.53 & 82.16  \\

\bottomrule

\end{tabular}%
}

\caption{Translation results on 17 languages from FLORES-101 test set. In this table, “x” denotes translation into any of the other 16 languages, excluding the source and target languages in each translation direction.}
\label{tab:translation_flores}
\end{table*}
\endgroup