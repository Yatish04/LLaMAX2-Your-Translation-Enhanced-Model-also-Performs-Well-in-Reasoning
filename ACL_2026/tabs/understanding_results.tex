% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table*}[!ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}l|c|c|c|c|c|c|c|c|c@{}}
\toprule
 \textbf{Models} & \textbf{HumanEval$+$} & \textbf{XNLI} & \textbf{MGSM} & \textbf{xIFEval} & \textbf{XStoryCloze} & \textbf{MathQA} & \textbf{XCOPA} & \textbf{XGPQA} & \textbf{XWinograd} \\
\midrule 

% \midrule
% \multicolumn{15}{c}{General LLMs} \\
% \midrule


% \midrule
% \multicolumn{15}{c}{Domain-Specialized LLMs} \\
% \midrule


% \midrule
% \multicolumn{15}{c}{Multilingual LLMs} \\
% \midrule

 
Aya-Expanse-8B & 40.24 & 45.53 & 14.51 & 40.46 & 64.80 & 37.69 & 56.36 & 20.85 & 74.67 \\
CodeLlama-7B & 31.71 & 40.69 & 1.78 & 30.51 & 56.45 & 28.71 & 54.58 & 14.40 & 76.42 \\
DeepSeek-Coder-V2-Lite & 74.39 & 42.30 & 30.62 & 43.41 & 62.69 & 45.16 & 59.27 & 22.86 & 80.24 \\
LLaMAX3-8B-Alpaca & 24.39 & 45.33 & 9.78 & 36.74 & 61.84 & 34.17 & 63.85 & 21.78 & 74.80 \\
Llama-3-8B & 54.27 & 42.14 & 55.38 & 65.32 & 59.50 & 27.24 & 60.89 & 27.25 & 71.09 \\
Llama-3.1-8B & 61.59 & 44.83 & 28.36 & 45.46 & 64.47 & 39.20 & 60.89 & 19.80 & 81.70 \\
% Llama-3.2-3B & 50.00 & 41.09 & 31.16 & 39.17 & 59.62 & 34.44 & 56.24 & 13.08 & 75.61 \\
% Qwen2.5-1.5B & 55.49 & 41.90 & 23.67 & 36.15 & 58.05 & 34.07 & 57.27 & 15.35 & 75.63 \\
Qwen2.5-14B & 73.17 & 39.64 & 39.78 & 58.35 & 66.72 & 48.48 & 65.33 & 32.41 & 82.72 \\
Qwen2.5-32B & 84.15 & 40.16 & 74.95 & 82.67 & 65.91 & 38.22 & 65.98 & 38.12 & 71.66 \\
% Qwen2.5-3B & 64.63 & 39.52 & 48.36 & 58.64 & 59.43 & 33.94 & 60.42 & 23.71 & 65.05 \\
Qwen2.5-7B & 75.00 & 39.84 & 34.80 & 57.64 & 63.25 & 40.27 & 62.84 & 26.19 & 80.58 \\
% Qwen2.5-Coder-1.5B & 64.63 & 41.43 & 29.45 & 43.84 & 54.15 & 30.08 & 53.78 & 17.76 & 62.46 \\
% Qwen2.5-Coder-14B & 85.98 & 43.37 & 64.00 & 69.10 & 60.31 & 39.93 & 60.55 & 27.94 & 73.27 \\
% Qwen2.5-Coder-32B & 83.54 & 42.16 & 68.29 & 81.42 & 63.55 & 37.22 & 62.42 & 32.64 & 68.33 \\
% Qwen2.5-Coder-3B & 79.88 & 41.95 & 42.00 & 51.88 & 56.81 & 34.64 & 57.55 & 21.53 & 67.36 \\
Qwen2.5-Coder-7B & 85.37 & 42.95 & 52.25 & 61.01 & 58.58 & 37.15 & 58.96 & 23.83 & 70.60 \\
% Qwen2.5-Math-1.5B & 28.05 & 34.93 & 11.64 & 23.67 & 48.66 & 44.89 & 52.67 & 10.27 & 57.59 \\
% Qwen2.5-Math-7B & 46.95 &  & 30.33 & 27.73 & 51.60 & 51.62 &  & 12.57 & 67.99 \\
% Qwen3-0.6B & 33.54 & 39.36 & 21.82 & 54.78 & 51.70 & 27.44 & 53.42 & 23.99 & 55.23 \\
% Qwen3-1.7B & 64.63 & 38.51 & 32.62 & 64.68 & 53.78 & 30.85 & 55.27 & 22.54 & 57.29 \\
% Qwen3-32B & 84.76 & 43.06 & 70.87 & 86.91 & 62.03 & 36.62 & 65.87 & 48.83 & 68.35 \\
% Qwen3-4B & 75.00 & 39.29 & 47.78 & 77.53 & 54.86 & 30.08 & 57.33 & 30.83 & 60.26 \\
Sailor2-8B-Chat & 39.63 & 38.30 & 34.84 & 43.01 & 63.56 & 40.64 & 24.11  & 24.11 & 81.64 \\
Tower-Plus-9B & 0.00 & 42.54 & 64.00 & 75.52 & 61.65 & 33.13 & 60.62 & 27.36 & 72.02 \\
TowerInstruct-7B-v0.1 & 19.51 & 40.45 & 3.75 & 30.19 & 59.11 & 29.25 & 56.98 & 15.77 & 78.40 \\
DeepSeek-Math-7B & 51.83 & 42.02 & 26.95 & 33.33 & 58.56 & 37.96 & 56.67 & 20.25 & 76.26 \\
Gemma-3-12B-IT & 0.00 & 33.33 & 0.04 & 25.19 & 52.81 & 20.57 & 50.00 & 0.00 & 51.63 \\
% gemma-3-1B-it & 0.00 & 39.13 & 1.24 & 30.92 & 53.67 & 24.05 & 54.98 & 15.55 & 53.70 \\
% gemma-3-4B-it & 0.00 & 33.33 & 0.11 & 24.92 & 52.81 & 20.57 & 50.00 & 0.10 & 51.63 \\
InternLM2-Math-7B & 28.05 & 38.60 & 39.24 & 35.63 & 55.68 & 26.20 & 55.45 & 18.12 & 61.50 \\
\midrule
Qwen3-8B & 76.83 & 42.44 & 44.87 & 81.68 & 58.08 & 32.80 & 60.40 & 35.85 & 63.79 \\
% Qwen3-8B-FT \\
% Qwen3-8B-Lora \\
\modelsmall & 76.22 & 44.79 & 50.36 & 80.49 & 59.24 & 32.93 & 61.44 & 34.26 & 67.95 \\
\midrule
Qwen3-14B & 85.37 & 43.05 & 52.18 & 85.64 & 58.73 & 34.71 & 61.87 & 41.43 & 60.58 \\
% Qwen3-14B-FT \\
% Qwen3-14B-Lora \\
\modellarge & 85.98 & 44.77 & 50.22 & 85.55 & 61.14 & 35.88 & 63.60 &	40.10 &	64.87 \\
\bottomrule
\end{tabular}%
}
\caption{Comparison of \modelseries and existing LLMs on reasoning and multilingual tasks.}
\label{tab:general_task}
\end{table*}
