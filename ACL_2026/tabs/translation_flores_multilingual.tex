\begingroup
\renewcommand{\arraystretch}{1} % Default value: 1
\begin{table*}[!ht]
\centering
\footnotesize
\resizebox{\linewidth}{!}{
\begin{tabular}{l|rr|rr|rr|rr|rr|rr|rr}

\toprule
& \multicolumn{2}{c|}{\textbf{x $\rightarrow$ en}} & \multicolumn{2}{c|}{\textbf{x $\rightarrow$ sw}} & \multicolumn{2}{c|}{\textbf{x $\rightarrow$ th}} & \multicolumn{2}{c|}{\textbf{x $\rightarrow$ bn}} & \multicolumn{2}{c|}{\textbf{x$\rightarrow$ zh}} & \multicolumn{2}{c|}{\textbf{x $\rightarrow$ ar}} & \multicolumn{2}{c}{\textbf{x $\rightarrow$ ko}} \\
& \textbf{spBLEU} & \textbf{xComet} & \textbf{spBLEU} & \textbf{xComet} & \textbf{spBLEU} & \textbf{xComet} & \textbf{spBLEU} & \textbf{xComet} & \textbf{spBLEU} & \textbf{xComet} & \textbf{spBLEU} & \textbf{xComet} & \textbf{spBLEU} & \textbf{xComet} \\
% Qwen3-14B & 36.92 & 91.98 & 5.87 & 15.33 & 32.40 & 80.12 & 17.50 & 68.09 & 28.71 & 83.95 & 24.01 & 79.75 & 18.77 & 82.19 \\

\midrule
 
TowerInstruct-7B-v0.1 & 29.26 & 72.56 & 0.71 & 37.34 & 0.48 & 53.90 & 0.25 & 59.70 & 17.02 & 62.16 & 0.48 & 58.62 & 13.4 & 62.28 \\
Hunyuan-MT-7B & 21.20 & 67.68 & 5.55 & 32.95 & 17.70 & 56.92 & 8.92 & 47.17 & 18.35 & 73.67 & 13.70 & 54.37 & 10.32 & 58.28 \\
Sailor2-8B-Chat & 0.52 & 17.81 & 0.67 & 17.09 & 24.12 & 60.84 & 2.42 & 20.67 & 16.69 & 60.53 & 5.60 & 31.41 & 4.46 & 37.03 \\
LLaMAX3-8B-Alpaca & 35.96 & 89.98 & 10.00 & 53.15 & 23.62 & 72.43 & 12.04 & 66.76 & 21.08 & 77.72 & 17.57 & 72.17 & 11.90 & 76.11 \\
Tower-Plus-9B & \textbf{40.12} & 91.74 & 2.45 & 20.80 & 18.71 & 53.76 & 2.47 & 58.16 & \textbf{30.37} & 82.96 & 9.66 & 48.73 & \textbf{22.36} & \textbf{85.53} \\
Aya-Expanse-8B & 33.13 & 79.28 & 1.49 & 8.91 & 6.42 & 19.81 & 4.94 & 25.08 & 23.53 & 70.67 & 23.77 & 70.21 & 17.71 & 70.53 \\
Aya-Expanse-32B & 39.72 & 88.63 & 2.60 & 16.53 & 15.16 & 40.65 & 11.93 & 53.76 & 27.93 & 80.70 & \textbf{28.63} & 81.70 & 21.71 & 82.79 \\

\midrule
Qwen3-8B & 35.24 & 89.89 & 3.49 & 12.52 & 29.85 & 75.47 & 14.14 & 59.91 & 26.88 & 81.65 & 21.73 & 75.18 & 16.20 & 76.48 \\
\modelsmall & 38.02 & 91.35 & 18.60 & 50.99 & 27.84 & 73.17 & 19.39 & 70.67 & 26.95 & 82.15 & 24.00 & 77.50 & 18.08 & 80.54 \\

\midrule
Qwen3-14B & 36.92 & 91.98 & 5.87 & 15.33 & 32.40 & 80.12 & 17.50 & 68.09 & 28.71 & 83.95 & 24.01 & 79.75 & 18.77 & 82.19 \\
\modellarge & 39.01 & \textbf{92.86} & \textbf{20.02} & \textbf{57.66} & \textbf{32.03} & \textbf{80.47} & \textbf{21.63} & \textbf{77.43} & 28.96 & \textbf{84.90} & 26.31 & \textbf{82.19} & 20.31 & 84.68 \\
\bottomrule




\toprule
& \multicolumn{2}{c|}{\textbf{en $\rightarrow$ x}} & \multicolumn{2}{c|}{\textbf{sw $\rightarrow$ x}} & \multicolumn{2}{c|}{\textbf{th $\rightarrow$ x}} & \multicolumn{2}{c|}{\textbf{bn $\rightarrow$ x}} & \multicolumn{2}{c|}{\textbf{zh $\rightarrow$ x}} & \multicolumn{2}{c|}{\textbf{ar $\rightarrow$ x}} & \multicolumn{2}{c}{\textbf{ko $\rightarrow$ x}} \\
& \textbf{spBLEU} & \textbf{xComet} & \textbf{spBLEU} & \textbf{xComet} & \textbf{spBLEU} & \textbf{xComet} & \textbf{spBLEU} & \textbf{xComet} & \textbf{spBLEU} & \textbf{xComet} & \textbf{spBLEU} & \textbf{xComet} & \textbf{spBLEU} & \textbf{xComet} \\


\midrule

TowerInstruct-7B-v0.1 & 18.26 & 67.80 & 2.67 & 17.33 & 3.82 & 29.22 & 1.78 & 21.56 & 10.89 & 69.26 & 7.95 & 39.09 & 11.26 & 70.69 \\
Hunyuan-MT-7B & 28.43 & 87.96 & 14.12 & 39.86 & 7.53 & 36.37 & 4.60 & 28.71 & 20.37 & 83.94 & 15.72 & 55.10 & 14.31 & 51.93 \\
Sailor2-8B-Chat & 16.03 & 54.11 & 1.76 & 14.08 & 4.30 & 33.86 & 5.72 & 30.13 & 3.83 & 28.77 & 7.29 & 34.26 & 7.18 & 36.11 \\
LLaMAX3-8B-Alpaca & 26.62 & 83.22 & 20.23 & \textbf{59.06} & 16.03 & 74.88 & 17.20 & 68.49 & 16.51 & 80.33 & 18.37 & 73.77 & 17.39 & 72.23 \\
Tower-Plus-9B & 28.83 & 79.85 & 18.38 & 46.19 & 19.02 & 70.29 & 18.64 & 62.92 & 19.39 & 75.55 & 21.72 & 69.28 & 20.68 & 71.93 \\
Aya-Expanse-8B & 25.75 & 68.36 & 7.90 & 16.43 & 11.39 & 40.78 & 11.29 & 36.77 & 17.85 & 65.45 & 20.21 & 60.86 & 18.41 & 60.74 \\
Aya-Expanse-32B & 30.21 & 78.30 & 16.72 & 38.82 & 18.25 & 64.11 & 19.37 & 62.04 & 21.26 & 75.21 & 24.71 & 71.16 & 22.07 & 70.84 \\


\midrule

Qwen3-8B & 28.86 & 80.27 & 13.61 & 32.32 & 19.68 & 72.64 & 19.38 & 66.51 & 20.82 & 78.90 & 22.85 & 72.27 & 20.23 & 71.79 \\
\modelsmall & 32.82 & 85.52 & 21.41 & 55.06 & 22.68 & 77.36 & 22.47 & 71.75 & 23.31 & 83.13 & 25.46 & 75.63 & 22.73 & 75.77 \\
\midrule
Qwen3-14B & 31.78 & 84.54 & 18.40 & 43.75 & 22.35 & 78.09 & 22.47 & 72.32 & 23.02 & 83.04 & 25.28 & 76.98 & 22.90 & 77.07 \\
\modellarge & \textbf{35.97} & \textbf{89.51} & \textbf{23.19} & 57.23 & \textbf{24.91} & \textbf{83.40} & \textbf{24.93} & \textbf{77.15} & \textbf{25.41} & \textbf{87.64} & \textbf{28.18} & \textbf{81.90} & \textbf{25.53} & \textbf{82.16}  \\

\bottomrule

\end{tabular}%
}

\caption{Comparison translation performance of \modelseries with Qwen3, and between Full Fine-Tuning~(FFT) and LoRA on the FLORES-101 test set covering 17 languages, with results for 7 representative languages shown in the table.
\modellarge delivers the best performance on 21 of the 28 reported metrics. In this table, “x” denotes translation into any of the other 16 languages, excluding the source and target languages in each translation direction. In this table, “x” denotes translation into any of the other 16 languages, excluding the source and target languages in each translation direction.}
\label{tab:translation_flores_multilingual}
\end{table*}
\endgroup
