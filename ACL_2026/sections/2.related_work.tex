\section{Related Work}
\subsection{Massively Multilingual Translation with Large Language Models}
Massively multilingual translation refers to building a single machine translation model to handle many language directions \cite{zhang-etal-2020-improving}. 
% In the neural machine translation (NMT) era, the mainstream approach to improve multilingual translation performance is to finetune the model on specific datasets after large scale parallel pretraining \cite{johnsonGooglesMultilingualNeural2017,aharoni-etal-2019-massively,zhang-etal-2020-improving,tang2020multilingualtranslationextensiblemultilingual,lin-etal-2021-learning,huang-etal-2023-knowledge}. This approach provides usable models, but relies heavily on parallel data. To overcome the lack of parallel data in low-resource language directions, work like NLLB  \cite{nllbteam2022languageleftbehindscaling} combines self-supervised objectives with translation training, which aligns with the idea of translation LLMs.
% The rise of LLMs pushes massively multilingual translation to a new level. 
Due to the multilingual and instruction-following nature of LLMs, they already show high translation performance in many directions without training~\cite{bawden-yvon-2023-investigating,zhu-etal-2024-multilingual} or with minimal training \cite{li-etal-2024-eliciting,cui-etal-2025-multilingual,MindMerger}. Based on this, specialized translation LLMs have been developed to do massive multilingual translation. For example, LLaMAX \cite{lu-etal-2024-llamax} and Tower \cite{alvesTowerOpenMultilingual2024a} apply continued pretraining and instruction tuning on the LLaMA-2 model \cite{touvron2023llama2openfoundation} with massive parallel and monolingual data, achieving comparable performance to specialized translation models. MT-R1-Zero \cite{feng2025mtr1zeroadvancingllmbasedmachine} adapts the R1-Zero reinforcement learning framework to the translation task and resulting a reasoning translation model. However, the models' general instruction-following and reasoning capabilities drop after the training, which weakens the advantage of using LLMs for translation. This work finds a simple, parameter-efficient but effective approach to train a translation LLM while keeping (and even improving) its general ability.

\subsection{Parameter-Efficient Finetuning}
There are many parameter-efficient finetuning~(PEFT) techniques for tuning LLMs with less resource and reduced catastrophic forgetting. According to \citeposs{hanParameterEfficientFineTuningLarge2024} survey, there are mainly four types of PEFT: (1) Additive, including adapters \cite{houlsbyParameterEfficientTransferLearning2019a,pfeiffer-etal-2021-adapterfusion,heUnifiedViewParameterEfficient2021} and soft prompts \cite{liPrefixTuningOptimizingContinuous2021,liu-etal-2022-p}; (2) Selective \cite{guo-etal-2021-parameter,sungTrainingNeuralNetworks2021,liao-etal-2023-parameter}; (3) Reparameterized, mainly the LoRA family \cite{huLoRALowRankAdaptation2021b,dettmersQLoRAEfficientFinetuning2023,liuDoRAWeightDecomposedLowRank2024,owodunni2025continually}; (4) Hybrid \cite{heUnifiedViewParameterEfficient2021,hu-etal-2023-llm}. Our proposed method belongs to selective PEFT.