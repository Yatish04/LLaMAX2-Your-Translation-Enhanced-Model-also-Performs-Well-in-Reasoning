\section{Introduction}

Large Language Models~(LLMs; \citealp{IntroducingGPT52025,IntroducingClaude4,comaniciGemini25Pushing2025,deepseek-aiDeepSeekR1IncentivizingReasoning2025a,teamKimiK2Open2025,ByteDanceSeed,yangQwen3TechnicalReport2025a}) excel in reasoning; however, translation-enhanced LLMs~\cite{alves2024tower,reiTowerBridgingGenerality2025, sunHunyuanLargeOpenSourceMoE2024, lu-etal-2024-llamax} face challenges in this area. As shown in Figure~\ref{fig:introduction_general}, Tower-Plus-9B~\cite{reiTowerBridgingGenerality2025}, a translation-enhanced model, significantly improves multilingual instruction-following capabilities, yet underperforms on reasoning, such as LiveCodeBench-V5~\cite{jainLiveCodeBenchHolisticContamination2024a} and AIME2025~\cite{ArtProblemSolving}.


The root of this dilemma lies in the limitations of the current training approach~\cite{reiTowerBridgingGenerality2025,lu-etal-2024-llamax,douSailor2SailingSouthEast2025,zhengHunyuanMTTechnicalReport2025}, which typically begins with a base model and then trains on large multilingual datasets. The preference for starting with a base model instead of an instruct model stems from the fact that full fine-tuning~(FFT) can result in catastrophic forgetting~\cite{li-etal-2024-revisiting,alexandrov-etal-2024-mitigating}.
% The root of this dilemma stems from the limitations of the current training recipe, which typically start with a base model and follow a process that includes continual pretraining~(CPT, \citealp{keContinualPretrainingLanguage2022}) on large multilingual datasets, followed by supervised fine-tuning~(SFT, \citealp{chungScalingInstructionFinetunedLanguage2024a,ouyangTrainingLanguageModelsFollow2022}), and finally uses proximal policy optimization~(PPO, \citealp{schulmanProximalPolicyOptimization2017}) / direct preference optimization~(DPO, \citealp{rafailovDirectPreferenceOptimization2023a}). Unlike previous work, we question this typical recipe from the starting model, data requirements, and training process.


\input{ACL_2026/images/introduction_general_capabilities}


\input{ACL_2026/images/introduction}

% Furthermore, starting with a base model complicates the achievement of instruct model performance in reasoning tasks because it requires extensive fine-tuning with carefully selected data. In contrast
Unlike previous work, \modelseries are built on Qwen3 instruct models rather than base models. Since fundamental reasoning skills like math and coding are universal, there is no need to learn basic concepts in multiple languages~\cite{benchmax:2025,gao2025thinkingmultilinguallyempowerllm}. Meanwhile, to mitigate catastrophic forgetting, we apply \method which effectively balances translation quality and reasoning capabilities without the need for extra parameters. It employs a two-phase tuning process, training the four layers closest to the embedding layer and the fifteen layers further away, which consistently yields significant improvements across various datasets and model backbones. As a result, \modelseries significantly reduce the reliance on large amounts of high-quality data.

% \textbf{1. Starting with an Instruct Model, not Base Model.} As shown in Figure~\ref{fig:introduction_main}, Tower-Plus-9B builds on Gemma2-9B-Base~\cite{teamGemma2Improving2024}, Sailor2-8B-Chat~\cite{douSailor2SailingSouthEast2025} starts with Qwen2.5-7B-Base~\cite{yangQwen25MathTechnicalReport2024}. Interestingly, their enhanced translation average performance on en$\rightarrow$x~\footnote{x includes Spanish, French, German, Russian, Bengali, Japanese, Thai, Swahili, Chinese, Telugu, Arabic, Korean, Serbian, Czech, Hungarian, and Vietnamese} is that of the corresponding instruct versions, Gemma2-9B-it and Qwen2.5-7B-Instruct. The key factors behind this result are~(1) instruct models are fine-tuned with carefully chosen data to improve the base models' abilities across various tasks. It can be challenging for many to achieve the same performance as an instruct model through post-training; (2) fundamental skills like math and coding are universal, so there's no need to learn basic concepts in multiple languages. Therefore, \modelseries are built on Qwen3 instruct models rather than base models. 

% In this exciting landscape, several models stand out: Tower plus \cite{reiTowerBridgingGenerality2025}, with a remarkable 32 billion CPT tokens, supports 27 languages based on the Gemma2-Base model \cite{teamGemma2Improving2024}. Sailor 2 \cite{douSailor2SailingSouthEast2025}, a heavyweight with 500 billion tokens, covers 13 languages, starting with Qwen2.5-Base \cite{qwenQwen25TechnicalReport2025}. Hunyuan-MT \cite{zhengHunyuanMTTechnicalReport2025} pushes the limits further with an impressive 1.3 trillion tokens, supporting 33 languages and launching with Hunyuan-7B-Base \cite{sunHunyuanLargeOpenSourceMoE2024}. Furthermore, LLaMAX \cite{lu-etal-2024-llamax}, boasting 60 billion tokens, excels with support for 100 languages, beginning its journey with Llama3-Base \cite{dubeyLlama3Herd2024}. On average, enhancing the performance of a language requires at least around 1 billion tokens during the CPT phase alone; not to mention the even higher quality data needed for the SFT, PPO/DPO, and RL training phases, which poses a significant challenge for low-resource languages.
 




% \modelseries are built on Qwen3 instruct models rather than base models. 
% When comparing base models to their instruct versions (such as Qwen3-8B-Base vs. Qwen3-8B-Instruct), several challenges arise that impact the performance of the final models. First, instruct models are fine-tuned with carefully selected data to enhance the capabilities of base models across diverse tasks. Achieving similar performance to an instruct model from a base model through post-training can be difficult for most people. Additionally, most general capabilities like mathematical rules and coding structures are universal, there is no need to learn basic arithmetic or programming concepts in multiple languages. Therefore, applying multilingual alignment to an instruct model can more effectively create a powerful multilingual model.  



Particularly, a small amount of parallel data is enough for translation-enhancment. As depicted in Figure~\ref{fig:introduction_main}~\footnote{x includes Spanish, French, German, Russian, Bengali, Japanese, Thai, Swahili, Chinese, Telugu, Arabic, Korean, Serbian, Czech, Hungarian, and Vietnamese}, Tower-Plus-9B has an impressive 32 billion tokens, LLaMAX~\cite{lu-etal-2024-llamax} features 60 billion tokens, Sailor2-8B-Chat~\cite{douSailor2SailingSouthEast2025} needs 500 billion tokens, and Hunyuan-MT~\cite{zhengHunyuanMTTechnicalReport2025} takes it even further with 1311 billion tokens. 
% On average, improving a language model requires about 1 billion tokens just for the continual pretraining phase. 
Not to mention, higher-quality data are needed for the supervised fine-tuning~\citealp{chungScalingInstructionFinetunedLanguage2024a,ouyangTrainingLanguageModelsFollow2022}), which poses a significant challenge for low-resource languages. Howerver, \modelseries utilize only 0.8 billion tokens from with our careful pre-processing. We standardize the original data from NLLB~\cite{nllbteam2022languageleftbehindscaling,schwenk-etal-2021-ccmatrix} and OPUS-100~\cite{tiedmann2012parallel} to a format-unified, clean, deduplicated, language-consistent, quality-controlled, and instructed-formatted dataset. 

\modelseries significantly improves translation performance, achieving over 15+ spBLEU points increase and 40+ xComet points in low-resource~(sw), with notable enhancements in high-resource translations. Utilizing small parallel data alone, it also demonstrates an average improvement of over 1 point across 7 multilingual tasks, including xIFEval~\cite{benchmax:2025}, MGSM~\cite{shiLanguageModelsAre2022b}, XGPQA~\cite{rein2024gpqa, benchmax:2025}, and so on. Furthermore, comprehensive testing on 15 popular reasoning datasets, such as BBEH~\cite{kazemi-etal-2025-big}, Livecodebench~\cite{jainLiveCodeBenchHolisticContamination2024a}, Olymmath~\cite{he-etal-2024-olympiadbench} and so on, shows that it surpasses existing translation-enhanced models and performs on par with Qwen3 instruct models. Our main contributions can be summarized as follows:
\begin{itemize}
[nosep,itemsep=2pt,leftmargin=0.3cm]
    \item We propose a new training recipe: using a small amount of parallel data for layer-selective tuning on an instruct model, which significantly reduces complexity and makes it more accessible for a wider range of languages.
    
    \item We introduce 2 open-sourced translation-enhanced \modelsmall and \modellarge, which maintaining reasoning capabilities.
    \item Extensive experiments on \modelseries and comprehensive benchmark evaluations demonstrate that we can achieve a balance between translation quality and reasoning capabilities.
\end{itemize}